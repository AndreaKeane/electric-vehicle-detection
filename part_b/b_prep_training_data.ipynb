{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import utilities as utils\n",
    "import warnings\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = Path('/Users/andreakeane/Documents/DataScience/GridCure_Problems/pickles/')\n",
    "test = pd.read_pickle(pickle_path / \"test.pkl\")\n",
    "train = pd.read_pickle(pickle_path / \"train.pkl\")\n",
    "labels = pd.read_pickle(pickle_path / \"labels.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485 Houses with EVs.\n",
      "1105 Houses without EVs.\n"
     ]
    }
   ],
   "source": [
    "# Determine houses with and without EVs\n",
    "temp = pd.DataFrame(index=labels.index)\n",
    "temp['sum'] = labels.sum(axis=1)\n",
    "temp['bool'] = np.where(temp['sum'] > 0, True, False)\n",
    "\n",
    "houses_wEV = temp.index[temp['bool'] == True].tolist()\n",
    "houses_woEV = temp.index[temp['bool'] == False].tolist()\n",
    "\n",
    "print(\"{} Houses with EVs.\".format(len(houses_wEV)))\n",
    "print(\"{} Houses without EVs.\".format(len(houses_woEV)))\n",
    "\n",
    "# Pickle the data for reference elsewhere\n",
    "# Lists, so we can't use Pandas pickling method\n",
    "utils.make_pickle(houses_wEV, pickle_path / \"houses_wEV.pkl\")\n",
    "utils.make_pickle(houses_woEV, pickle_path / \"houses_woEV.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Outliers: 37\n",
      "Percent Removed: 2.33%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine outliers\n",
    "outliers = utils.classify_outliers(train)\n",
    "\n",
    "# Drop Outliers\n",
    "train_clean = train.drop(outliers.index)\n",
    "labels_clean = labels.drop(outliers.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Feature Engineering\n",
    "A series with significant dependence among values. In this case we need to use some statistical models like ARIMA to forecast the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total anticipated rows: 4471920\n",
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n"
     ]
    }
   ],
   "source": [
    "# Feature Setup\n",
    "# Difference from preceding interval\n",
    "feat_diff = train.diff(axis=1)\n",
    "\n",
    "# 8-hour average\n",
    "feat_8h_avg = train.rolling(window=(8*2), axis=1).mean()\n",
    "\n",
    "# 24-hour average\n",
    "feat_24h_avg = train.rolling(window=(24*2), axis=1).mean()\n",
    "\n",
    "# 24-hour min\n",
    "feat_24h_min = train.rolling(window=(24*2), axis=1).min()\n",
    "\n",
    "# 24-hour max\n",
    "feat_24h_max = train.rolling(window=(24*2), axis=1).max()\n",
    "\n",
    "# 72-hour average\n",
    "feat_72h_avg = train.rolling(window=(72*2), axis=1).mean()\n",
    "\n",
    "\n",
    "features = pd.DataFrame(index=train_clean.stack().index)\n",
    "print(\"Total anticipated rows: {}\".format(features.shape[0]))\n",
    "for i, index in enumerate(features.index.tolist()): \n",
    "    house_id, interval = index\n",
    "    features.at[index, 'value'] = train[interval].loc[house_id]\n",
    "    features.at[index, 'diff'] = feat_diff[interval].loc[house_id]\n",
    "    features.at[index, 'h8_avg'] = feat_8h_avg[interval].loc[house_id]\n",
    "    features.at[index, 'h24_avg'] = feat_24h_avg[interval].loc[house_id]\n",
    "    features.at[index, 'h24_min'] = feat_24h_min[interval].loc[house_id]\n",
    "    features.at[index, 'h24_max'] = feat_24h_max[interval].loc[house_id]\n",
    "    features.at[index, 'h72_avg'] = feat_72h_avg[interval].loc[house_id]\n",
    "    features.at[index, 'label'] = labels_clean[interval].loc[house_id]  \n",
    "    if i % 1000000 == 0: print(i)\n",
    "\n",
    "# Create combinations of features\n",
    "features['diff_2'] = features['diff']**2\n",
    "features['diff_3'] = features['diff']**3\n",
    "features['diff_5'] = features['diff']**5\n",
    "\n",
    "features.to_pickle(pickle_path / \"features.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a clean version of features\n",
    "features = pd.read_pickle(pickle_path / \"features.pkl\")\n",
    "\n",
    "# Split into features and labels\n",
    "X = features.drop(columns=['label'])\n",
    "y = features['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222079 Rows were dropped for NA reasons.\n",
      "(4249841, 10)\n",
      "(4249841,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Drop NA-containing rows from X\n",
    "before = set(X.index.tolist())\n",
    "X = X.dropna(axis=0)\n",
    "after = set(X.index.tolist())\n",
    "removed_id = before - after\n",
    "print(\"{} Rows were dropped for NA reasons.\".format(len(removed_id)))\n",
    "\n",
    "# Drop corresponsing rows from y\n",
    "y = y.drop(index=removed_id)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "X.to_pickle(pickle_path / \"X_all.pkl\")\n",
    "y.to_pickle(pickle_path / \"y_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
